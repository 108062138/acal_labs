{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "print('==================================================================================================================================================')\n",
    "print('= HW 2-3-3 replace the Linear layers in the AlexNet with the equivalent subgraphs                                                                =')\n",
    "print('==================================================================================================================================================')\n",
    "\n",
    "batch_size = 64\n",
    "input_channel = 3\n",
    "input_height = 224\n",
    "input_width = 224\n",
    "input_tensor = torch.randn(batch_size, input_channel, input_height, input_width) # Example input, replace with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_param_layer(nn.Module):\n",
    "    def __init__(self, B, M, K, N, chunk_row_size, chunk_col_size, last, bias=None):\n",
    "        super(my_param_layer, self).__init__()\n",
    "        self.B = B\n",
    "        self.M = (chunk_row_size - M % chunk_row_size) % chunk_row_size + M\n",
    "        self.K = (chunk_col_size - K % chunk_col_size) % chunk_col_size + K\n",
    "        self.N = (chunk_row_size - N % chunk_row_size) % chunk_row_size + N\n",
    "        self.chunk_row_size = chunk_row_size\n",
    "        self.chunk_col_size = chunk_col_size\n",
    "        self.bias = bias\n",
    "        self.last = last\n",
    "    def my_split(self, A, target_height, target_width, dh, dc):\n",
    "        # Calculate the current height and width\n",
    "        current_height, current_width = A.shape\n",
    "\n",
    "        # Calculate the required padding to reach the target dimensions\n",
    "        padding_height = max(0, target_height - current_height)\n",
    "        padding_width = max(0, target_width - current_width)\n",
    "\n",
    "        # Apply padding to the bottom and right to match the target dimensions\n",
    "        # Padding format is (left, right, top, bottom)\n",
    "        padded_A = torch.nn.functional.pad(A, (0, padding_width, 0, padding_height), \"constant\", 0)\n",
    "\n",
    "        # Debugging print, can be removed later\n",
    "        if padded_A.shape != A.shape:\n",
    "            print(f\"Original shape: {A.shape}, padded into: {padded_A.shape}\")\n",
    "            print(f\"Target height: {target_height}, Target width: {target_width}\")\n",
    "            print(f\"dh: {dh}, dc: {dc}\")\n",
    "\n",
    "        ls = []\n",
    "        for i in range(0, padded_A.shape[0], dh):\n",
    "            tmp = []\n",
    "            for j in range(0, padded_A.shape[1], dc):\n",
    "                block = padded_A[i:i+dh, j:j+dc]\n",
    "                tmp.append(block)\n",
    "            ls.append(tmp)\n",
    "        return ls\n",
    "    def matmul(self, A):\n",
    "        subAs = self.my_split(A,      self.M, self.K, self.chunk_row_size, self.chunk_col_size)\n",
    "        subBs = self.my_split(self.B, self.K, self.N, self.chunk_col_size, self.chunk_row_size)\n",
    "        # res = torch.zeros(self.M, self.N)\n",
    "        final_result = []\n",
    "        for i in range(0, self.M//self.chunk_row_size):\n",
    "            row_result = []\n",
    "            for j in range(0, self.N//self.chunk_row_size):\n",
    "                psum = torch.zeros(self.chunk_row_size, self.chunk_row_size)\n",
    "                for k in range(0, self.K//self.chunk_col_size):\n",
    "                    psum += torch.matmul(subAs[i][k], subBs[k][j])\n",
    "                row_result.append(psum)\n",
    "            row_result = torch.cat(row_result, dim = 1)\n",
    "            final_result.append(row_result)\n",
    "        final_result = torch.cat(final_result, dim = 0)\n",
    "        return final_result\n",
    "\n",
    "    def forward(self, A):\n",
    "        gemm = self.matmul(A)\n",
    "        if self.bias != None:\n",
    "            print('inside forward, gemm:',gemm.shape)\n",
    "            print('inside forward, bias', self.bias.shape)\n",
    "            if self.bias.shape[0] != gemm.shape[1]:\n",
    "                gemm[:, :self.bias.shape[0]] += self.bias\n",
    "                gemm = gemm[:, :self.bias.shape[0]]\n",
    "            else:\n",
    "                gemm += self.bias\n",
    "        return gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/popo/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/popo/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "alexnet_origin = models.alexnet(pretrained=True)\n",
    "alexnet_modified = models.alexnet(pretrained=True)\n",
    "\n",
    "alexnet_modified.classifier[1] = my_param_layer(alexnet_origin.classifier[1].weight.data.t(), batch_size, 9216, 4096, 64, 64, False, alexnet_origin.classifier[1].bias.data)\n",
    "alexnet_modified.classifier[4] = my_param_layer(alexnet_origin.classifier[4].weight.data.t(), batch_size, 4096, 4096, 64, 64, False, alexnet_origin.classifier[4].bias.data)\n",
    "alexnet_modified.classifier[6] = my_param_layer(alexnet_origin.classifier[6].weight.data.t(), batch_size, 4096, 1000, 64, 64, True, alexnet_origin.classifier[6].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside forward, gemm: torch.Size([64, 4096])\n",
      "inside forward, bias torch.Size([4096])\n",
      "inside forward, gemm: torch.Size([64, 4096])\n",
      "inside forward, bias torch.Size([4096])\n",
      "Original shape: torch.Size([4096, 1000]), padded into: torch.Size([4096, 1024])\n",
      "Target height: 4096, Target width: 1024\n",
      "dh: 64, dc: 64\n",
      "inside forward, gemm: torch.Size([64, 1024])\n",
      "inside forward, bias torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "alexnet_modified.eval()\n",
    "alexnet_origin.eval()\n",
    "\n",
    "converted_output = alexnet_modified(input_tensor)\n",
    "sample_output = alexnet_origin(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178605/1972142540.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  padding_height = max(0, target_height - current_height)\n",
      "/tmp/ipykernel_178605/1972142540.py:18: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  padding_width = max(0, target_width - current_width)\n",
      "/tmp/ipykernel_178605/1972142540.py:25: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if padded_A.shape != A.shape:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside forward, gemm: torch.Size([64, 4096])\n",
      "inside forward, bias torch.Size([4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178605/1972142540.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.bias.shape[0] != gemm.shape[1]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside forward, gemm: torch.Size([64, 4096])\n",
      "inside forward, bias torch.Size([4096])\n",
      "Original shape: torch.Size([4096, 1000]), padded into: torch.Size([4096, 1024])\n",
      "Target height: 4096, Target width: 1024\n",
      "dh: 64, dc: 64\n",
      "inside forward, gemm: torch.Size([64, 1024])\n",
      "inside forward, bias torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/popo/.local/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/popo/.local/lib/python3.10/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/popo/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "torch.onnx.export(alexnet_modified, input_tensor, \"modified_alexnet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================================================\n",
      "= HW 2-3-4 Correctness verification                                                                                                              =\n",
      "==================================================================================================================================================\n",
      "sum of each entry abs. diff: 0.014157585799694061, which is contributed by 9633792 entries\n",
      "converted output:  tensor([[ 0.1253, -0.9570],\n",
      "        [ 0.1109, -1.4149]], grad_fn=<SliceBackward0>)\n",
      "sample output:  tensor([[ 0.1253, -0.9570],\n",
      "        [ 0.1109, -1.4149]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('==================================================================================================================================================')\n",
    "print('= HW 2-3-4 Correctness verification                                                                                                              =')\n",
    "print('==================================================================================================================================================')\n",
    "\n",
    "error = torch.sum(torch.abs(converted_output - sample_output))\n",
    "num_entry = batch_size * input_channel * input_height * input_width\n",
    "\n",
    "print('sum of each entry abs. diff: {}, which is contributed by {} entries'.format(error, num_entry))\n",
    "\n",
    "print('converted output: ',converted_output[:2, :2])\n",
    "print('sample output: ',sample_output[:2, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
